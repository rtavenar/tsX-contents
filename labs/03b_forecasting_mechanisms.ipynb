{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f29923",
   "metadata": {},
   "source": [
    "# Forecasting Mechanisms: Multi-step Ahead and Probabilistic Forecasting\n",
    "\n",
    "In this lab, you will explore different strategies for multi-step ahead forecasting\n",
    "and implement probabilistic forecasting models. You will learn about:\n",
    "- Direct vs. autoregressive forecasting approaches\n",
    "- Curriculum learning and scheduled sampling for autoregressive models\n",
    "- Probabilistic losses (Gaussian NLL, quantile loss)\n",
    "- Uncertainty visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d54bd3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6bec9f",
   "metadata": {},
   "source": [
    "## Part 1: Direct vs. Autoregressive Forecasting\n",
    "\n",
    "In this section, you will compare two approaches to multi-step ahead forecasting:\n",
    "1. **Direct forecasting**: Predict all future steps in one forward pass\n",
    "2. **Autoregressive forecasting**: Iteratively predict one step at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4645c6",
   "metadata": {},
   "source": [
    "Below is a dataloader for the univariate ETTh1 dataset from previous labs, code for a direct\n",
    "forecasting model as well as a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e24f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastingDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Windowed univariate forecasting dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 csv_path: str, \n",
    "                 window: int, \n",
    "                 horizon: int, \n",
    "                 target_col: int = -1,\n",
    "                 start: int = 0,\n",
    "                 end: int = None):\n",
    "        super().__init__()\n",
    "        raw = numpy.loadtxt(csv_path, delimiter=\",\", skiprows=1, usecols=target_col)\n",
    "        series = raw.astype(numpy.float32)\n",
    "        if end is None:\n",
    "            series = series[start:]\n",
    "        else:\n",
    "            series = series[start:end]\n",
    "        self.window = window\n",
    "        self.horizon = horizon\n",
    "        self.series = series\n",
    "        self.max_start = len(series) - window - horizon + 1\n",
    "        if self.max_start < 1:\n",
    "            raise ValueError(\"Window + horizon larger than available series length\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_start\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        start = idx\n",
    "        past = self.series[start : start + self.window]\n",
    "        future = self.series[start + self.window : start + self.window + self.horizon]\n",
    "        past = torch.from_numpy(past)  # shape: (window,)\n",
    "        future = torch.from_numpy(future)  # shape: (horizon,)\n",
    "        return past, future\n",
    "\n",
    "\n",
    "def build_dataloader(csv_path: str, \n",
    "                     window: int, \n",
    "                     horizon: int, \n",
    "                     batch_size: int = 32, \n",
    "                     shuffle: bool = True):\n",
    "    \"\"\"Create a DataLoader emitting `(past, horizon)` batches.\"\"\"\n",
    "    dataset = ForecastingDataset(csv_path=csv_path, \n",
    "                                 window=window, \n",
    "                                 horizon=horizon)\n",
    "    n = len(dataset)\n",
    "    n_train = int(0.8 * n)\n",
    "    train_dataset = ForecastingDataset(csv_path=csv_path, \n",
    "                                      window=window, \n",
    "                                      horizon=horizon,\n",
    "                                      end=n_train)\n",
    "    valid_dataset = ForecastingDataset(csv_path=csv_path, \n",
    "                                       window=window, \n",
    "                                       horizon=horizon,\n",
    "                                       start=n_train)\n",
    "    train_dl = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=shuffle, \n",
    "                                           drop_last=False)\n",
    "    valid_dl = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False, \n",
    "                                           drop_last=False)\n",
    "    return train_dl, valid_dl\n",
    "\n",
    "\n",
    "# Direct forecasting model: predicts all steps at once\n",
    "class DirectForecaster(nn.Module):\n",
    "    \"\"\"Direct forecasting: predict all horizon steps in one forward pass.\"\"\"\n",
    "    \n",
    "    def __init__(self, window: int, horizon: int, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.window = window\n",
    "        self.horizon = horizon\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(window, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, horizon)\n",
    "        )\n",
    "    \n",
    "    def forward(self, past):\n",
    "        # past: (batch, window)\n",
    "        # output: (batch, horizon)\n",
    "        return self.net(past)\n",
    "\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train_epoch(model, dataloader, optimizer, criterion, use_teacher_forcing=False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for past, future in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        if isinstance(model, AutoregressiveForecaster):\n",
    "            pred = model(past, use_ground_truth=use_teacher_forcing, ground_truth=future)\n",
    "        else:\n",
    "            pred = model(past)\n",
    "        loss = criterion(pred, future)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * past.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for past, future in dataloader:\n",
    "        if isinstance(model, AutoregressiveForecaster):\n",
    "            pred = model(past, use_ground_truth=False)\n",
    "        else:\n",
    "            pred = model(past)\n",
    "        loss = criterion(pred, future)\n",
    "        total_loss += loss.item() * past.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def train_and_valid_loop(model, train_dl, valid_dl, optimizer, criterion, n_epochs, \n",
    "                         use_teacher_forcing=False):\n",
    "    logs = {\"train_loss\": [], \"valid_loss\": []}\n",
    "    print(model.__class__.__name__)\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_epoch(model, train_dl, optimizer, criterion, use_teacher_forcing)\n",
    "        logs[\"train_loss\"].append(train_loss)\n",
    "        valid_loss = eval_epoch(model, valid_dl, criterion)\n",
    "        logs[\"valid_loss\"].append(valid_loss)\n",
    "        print(f\"Epoch {epoch:02d} | train={train_loss:.4f} | valid={valid_loss:.4f}\")\n",
    "    return logs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2df7d9",
   "metadata": {},
   "source": [
    "**Question 1.** Implement an autoregressive forecasting model using the code template below.\n",
    "The model should allow:\n",
    "- teacher forcing\n",
    "- injection of ground truth in the sliding window fed as input of the model\n",
    "- curriculum learning (which could be implemented by providing only the first \n",
    "  few future steps in the `ground_truth` tensor)\n",
    "- scheduled sampling through the specification of a probability `p`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f274df8e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Autoregressive forecasting model: predicts one step at a time\n",
    "class AutoregressiveForecaster(nn.Module):\n",
    "    \"\"\"Autoregressive forecasting: iteratively predict one step at a time.\"\"\"\n",
    "    \n",
    "    def __init__(self, window: int, horizon: int, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.window = window\n",
    "        self.horizon = horizon\n",
    "        # Model that predicts one step ahead\n",
    "        self.step_model = nn.Sequential(\n",
    "            nn.Linear(window, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, past, use_ground_truth=False, ground_truth=None, p=None):\n",
    "        predictions = []\n",
    "        current_window = past.clone()  # (batch, window)\n",
    "        \n",
    "        for h in range(self.horizon):\n",
    "            # TODO: your code goes here\n",
    "            pass\n",
    "            \n",
    "            # Shift window: remove first element, add prediction\n",
    "            # current_window = torch.cat([current_window[:, 1:], next_value], dim=1)\n",
    "        \n",
    "        return torch.stack(predictions, dim=1)  # (batch, horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da7ae6",
   "metadata": {},
   "source": [
    "## Part 2: Curriculum Learning for Autoregressive Models\n",
    "\n",
    "Curriculum learning gradually increases the prediction horizon during training,\n",
    "helping the model learn to handle its own predictions progressively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b524bb9",
   "metadata": {},
   "source": [
    "**Question 2.** Implement a curriculum learning training loop for the autoregressive model:\n",
    "- Start by training to predict 1 step ahead\n",
    "- Gradually increase to 2, 4, 8, ... steps ahead\n",
    "- Finally train on the full horizon\n",
    "- Compare with the baseline autoregressive model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971548ce",
   "metadata": {},
   "source": [
    "## Part 3: Scheduled Sampling\n",
    "\n",
    "Scheduled sampling randomly replaces ground truth with model predictions during\n",
    "training, with the probability of using predictions increasing over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d1ceca",
   "metadata": {},
   "source": [
    "**Question 3.** Implement scheduled sampling for the autoregressive model:\n",
    "- Start with probability p=1.0 (always use ground truth)\n",
    "- Gradually decrease p to 0.0 (always use predictions)\n",
    "- Compare with baseline autoregressive model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8970f0c9",
   "metadata": {},
   "source": [
    "## Part 4: Probabilistic Forecasting with Gaussian NLL\n",
    "\n",
    "Instead of predicting point estimates, probabilistic models predict distributions.\n",
    "We'll start with a Gaussian distribution parameterized by mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faeff17",
   "metadata": {},
   "source": [
    "**Question 4.** Implement a probabilistic forecasting model that outputs both mean\n",
    "and (log-)variance, and train it using negative log-likelihood (NLL) loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345830f5",
   "metadata": {},
   "source": [
    "**Question 5.** Visualize the probabilistic forecasts with uncertainty intervals.\n",
    "Show the mean prediction along with confidence intervals (e.g., 50% and 90%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbf4b43",
   "metadata": {},
   "source": [
    "## Part 5: Quantile Regression\n",
    "\n",
    "Quantile regression predicts multiple quantiles simultaneously, providing another\n",
    "approach to uncertainty quantification without distributional assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc8709a",
   "metadata": {},
   "source": [
    "**Question 6.** Implement quantile regression for forecasting using the below implementation for the quantile loss:\n",
    "- Predict multiple quantiles (e.g., 10th, 50th, 90th percentiles)\n",
    "- Visualize the quantile predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f04739",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def quantile_loss(predictions, target, quantiles):\n",
    "    \"\"\"\n",
    "    Quantile loss (pinball loss).\n",
    "    \n",
    "    Args:\n",
    "        predictions: (batch, horizon, n_quantiles) - predicted quantiles\n",
    "        target: (batch, horizon) - ground truth values\n",
    "        quantiles: list of quantile values (e.g., [0.1, 0.5, 0.9])\n",
    "    Returns:\n",
    "        loss: scalar\n",
    "    \"\"\"\n",
    "    target = target.unsqueeze(-1)  # (batch, horizon, 1)\n",
    "    errors = target - predictions  # (batch, horizon, n_quantiles)\n",
    "    \n",
    "    quantiles_tensor = torch.tensor(quantiles, device=predictions.device).view(1, 1, -1)\n",
    "    \n",
    "    loss = torch.max(\n",
    "        quantiles_tensor * errors,\n",
    "        (quantiles_tensor - 1) * errors\n",
    "    )\n",
    "    return loss.mean()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
