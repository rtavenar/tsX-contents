{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8353ba24",
   "metadata": {},
   "source": [
    "# Transformers for Time Series Forecasting\n",
    "\n",
    "In this lab, you will implement and experiment with transformer-based models for time series\n",
    "forecasting. You will learn about:\n",
    "- RevIN (Reversible Instance Normalization) for handling distribution shifts\n",
    "- PatchTST: a patch-based transformer architecture for time series\n",
    "- Training with SAM (Sharpness-Aware Minimization) loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468eb7a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dc03c8",
   "metadata": {},
   "source": [
    "## Part 1: Unified Data Loaders\n",
    "\n",
    "In this section, you will create data loaders for ETTh1, reusing code from previous lab sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72da2196",
   "metadata": {},
   "source": [
    "**Question 1.** Create a unified data loader function. It should:\n",
    "- Support the natively multivariate `ETTh1Dataset` from Lab 2 \n",
    "  (multivariate inputs, univariate outputs)\n",
    "- Do not scale input features (you will use RevIn layers in your models for that)\n",
    "- Return training and validation data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7672fe",
   "metadata": {},
   "source": [
    "## Part 2: RevIN (Reversible Instance Normalization)\n",
    "\n",
    "RevIN is a normalization technique that helps transformers handle distribution shifts\n",
    "in time series data. It normalizes each instance (time series) independently and\n",
    "can be reversed after processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f24c00",
   "metadata": {},
   "source": [
    "**Question 2.** Implement the RevIN module:\n",
    "- Normalize input by subtracting mean and dividing by standard deviation (per instance)\n",
    "- Store the statistics to reverse the normalization after processing\n",
    "- Define learnable scaling parameters $\\gamma$ and $\\beta$\n",
    "- Support both forward (normalize) and reverse (denormalize) operations\n",
    "- Allow specification of a target channel for the multivariate-to-univariate \n",
    "  forecasting use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8187cc71",
   "metadata": {},
   "source": [
    "**Question 3.** Visualize the impact of this RevIN normalization/denormalization on a small set of time series from your data loader. Focus your visualizations on the target feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ba80dd",
   "metadata": {},
   "source": [
    "**Question 4.** Now visualize the impact of this RevIN normalization/denormalization at the distribution scale. Once again, focus on the target feature and plot training and validation empirical distributions before/after normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf9efd",
   "metadata": {},
   "source": [
    "## Part 3: PatchTST\n",
    "\n",
    "PatchTST (Patch-based Time Series Transformer) divides the input time series into\n",
    "patches and processes them with a transformer. This approach is more efficient than\n",
    "processing individual time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f15961e",
   "metadata": {},
   "source": [
    "**Question 5.** Implement a minimal PatchTST model for univariate forecasting  \n",
    "*At this stage, keep the simple channel-mixing implementation (no channel independence yet).*\n",
    "- Divide the input sequence into patches of a given length\n",
    "- Project patches to a model dimension\n",
    "- Add learnable positional encodings\n",
    "- Apply a standard transformer encoder \n",
    "  (use `nn.TransformerEncoder` and `nn.TransformerEncoderLayer` classes)\n",
    "- Mean-pool patch representations and predict the forecast horizon using a linear head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c733e11f",
   "metadata": {},
   "source": [
    "**Question 6.** Compare performance of the following models on the ETTh1 dataset:\n",
    "- PatchTST without RevIN and using patches of size 1\n",
    "- PatchTST with RevIN and using patches of size 1\n",
    "- PatchTST without RevIN and using patches of size 16\n",
    "- PatchTST with RevIN and using patches of size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412041c2",
   "metadata": {},
   "source": [
    "**Question 6bis.** Channel independence in PatchTST  \n",
    "One hallmark of PatchTST is channel-independent patch processing (depthwise patch embedding), which is not implemented above.  \n",
    "- Implement a channel-independent variant (one embedding per channel, no cross-channel mixing before the transformer). The typical implementation trick for this is to reorganize tensor dimensions such that independent channels can be processed as if they were independent time series, such that the batch dimension becomes B*C.\n",
    "- Compare its performance with the mixed-channel version from Question 6 on ETTh1.  \n",
    "- Discuss when channel independence helps or hurts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58915280",
   "metadata": {},
   "source": [
    "# Part 4: Training with SAM (Sharpness-Aware Minimization)\n",
    "\n",
    "Sharpness-Aware Minimization (SAM) improves generalization by encouraging\n",
    "solutions that lie in flat regions of the loss landscape.\n",
    "\n",
    "In this part:\n",
    "- You are GIVEN an implementation of the SAM optimizer\n",
    "- You must implement a SAM training step\n",
    "- You must adapt the training loop accordingly\n",
    "\n",
    "Important notes:\n",
    "- The loss function is NOT modified\n",
    "- The validation loop remains unchanged\n",
    "- Only the training step differs from standard optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e027d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    \"\"\"Sharpness-Aware Minimization optimizer wrapper.\"\"\"\n",
    "    \n",
    "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "        \n",
    "        defaults = dict(rho=rho, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "        \n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "            \n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                e_w = p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "        \n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
    "        \n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "        \n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "    \n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device\n",
    "        norm = torch.norm(\n",
    "            torch.stack([\n",
    "                p.grad.norm(p=2).to(shared_device)\n",
    "                for group in self.param_groups for p in group[\"params\"]\n",
    "                if p.grad is not None\n",
    "            ]),\n",
    "            p=2\n",
    "        )\n",
    "        return norm\n",
    "    \n",
    "    def step(self, closure=None):\n",
    "        raise NotImplementedError(\"SAM doesn't work like the other optimizers, you should first call `first_step` and then `second_step`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04493745",
   "metadata": {},
   "source": [
    "**Question 7.** Implement a SAM training step\n",
    "The SAM optimizer exposes two methods:\n",
    "  - optimizer.first_step()\n",
    "  - optimizer.second_step()\n",
    "\n",
    "Implement a function `sam_step` that:\n",
    "  1. Computes the loss and gradients at the current parameters\n",
    "  2. Calls `first_step()` to move to a nearby point of higher loss\n",
    "  3. Recomputes the loss and gradients at the perturbed parameters\n",
    "  4. Calls `second_step()` to update the model\n",
    "\n",
    "The function should return the final loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c8b2e",
   "metadata": {},
   "source": [
    "**Question 8.** Implement a training epoch using SAM\n",
    "\n",
    "Using the `sam_step` function, implement a training epoch.\n",
    "The structure should be similar to the standard training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8497b8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "**Question 9.** Full training loop with SAM\n",
    "\n",
    "Complete the training-and-validation loop using the SAM-based training epoch.\n",
    "The validation loop remains unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d15cf4d",
   "metadata": {},
   "source": [
    "**Question 8.** Compare the performance of:\n",
    "- PatchTST (Adam)\n",
    "- PatchTST (SAM)\n",
    "\n",
    "Visualize the training curves and some example forecasts."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
