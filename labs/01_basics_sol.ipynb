{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d34ed9a",
   "metadata": {},
   "source": [
    "# Basics of time series forecasting\n",
    "\n",
    "This lab introduces the fundamentals of time series forecasting with deep learning. \n",
    "You will learn how to build data loaders for time series data, implement simple \n",
    "autoregressive models, and experiment with preprocessing techniques such as \n",
    "standardization and differencing. The lab covers training and evaluation procedures, \n",
    "and compares the impact of different modeling choices on forecasting performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7588e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710300f9",
   "metadata": {},
   "source": [
    "## Part 1: Data loaders\n",
    "\n",
    "In this section, you will load a standard time series forecasting dataset and prepare a data loader for it.\n",
    "\n",
    "To begin, visit <https://github.com/zhouhaoyi/ETDataset> and download the ETTh1 dataset as a CSV file.\n",
    "\n",
    "**Question 1.** Visualize the dataset, focusing on the univariate time series corresponding to the target variable. Do you observe a trend? Periodicity? Any abnormal segments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50213a21",
   "metadata": {
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "raw = numpy.loadtxt(\"data/ETTh1.csv\", delimiter=\",\", skiprows=1, usecols=-1)\n",
    "series = raw.astype(numpy.float32)\n",
    "plt.plot(series, color='blue')\n",
    "plt.xlabel(\"Time (hours)\")\n",
    "plt.ylabel(\"Oil Temperature (Celsius degrees)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e16062",
   "metadata": {
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "plt.plot(series[:500], color='blue')\n",
    "plt.xlabel(\"Time (hours)\")\n",
    "plt.ylabel(\"Oil Temperature (Celsius degrees)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b17c5",
   "metadata": {},
   "source": [
    "**Question 2.** Implement a PyTorch `DataLoader` that reads the CSV file at initialization time, allows you to specify the past window length and forecast horizon, and provides batches of `(past, horizon)` pairs for the univariate forecasting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca2c06",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "class ForecastingDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Windowed univariate forecasting dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 csv_path: str, \n",
    "                 window: int, \n",
    "                 horizon: int, \n",
    "                 target_col: int = -1):\n",
    "        super().__init__()\n",
    "        raw = numpy.loadtxt(csv_path, delimiter=\",\", skiprows=1, usecols=target_col)\n",
    "        series = raw.astype(numpy.float32)\n",
    "        self.window = window\n",
    "        self.horizon = horizon\n",
    "        self.series = series\n",
    "        self.max_start = len(series) - window - horizon + 1\n",
    "        if self.max_start < 1:\n",
    "            raise ValueError(\"Window + horizon larger than available series length\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_start\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        start = idx\n",
    "        past = self.series[start : start + self.window]\n",
    "        future = self.series[start + self.window : start + self.window + self.horizon]\n",
    "        past = torch.from_numpy(past)  # shape: (window,)\n",
    "        future = torch.from_numpy(future)  # shape: (horizon,)\n",
    "        return past, future\n",
    "\n",
    "\n",
    "def build_dataloader(csv_path: str, \n",
    "                     window: int, \n",
    "                     horizon: int, \n",
    "                     batch_size: int = 32, \n",
    "                     shuffle: bool = True):\n",
    "    \"\"\"Create a DataLoader emitting `(past, horizon)` batches.\"\"\"\n",
    "    dataset = ForecastingDataset(csv_path=csv_path, \n",
    "                                 window=window, \n",
    "                                 horizon=horizon)\n",
    "    return torch.utils.data.DataLoader(dataset, \n",
    "                                       batch_size=batch_size, \n",
    "                                       shuffle=shuffle, \n",
    "                                       drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaacce7",
   "metadata": {},
   "source": [
    "**Question 3.** Improve your `build_dataloader` function above to build both a training data loader and a validation data loader. What would be appropriate choices for a clean separation between training and validation datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e1268e",
   "metadata": {
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "class ForecastingDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Windowed univariate forecasting dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 csv_path: str, \n",
    "                 window: int, \n",
    "                 horizon: int, \n",
    "                 target_col: int = -1,\n",
    "                 start: int = 0,\n",
    "                 end: int = None):\n",
    "        super().__init__()\n",
    "        raw = numpy.loadtxt(csv_path, delimiter=\",\", skiprows=1, usecols=target_col)\n",
    "        series = raw.astype(numpy.float32)\n",
    "        if end is None:\n",
    "            series = series[start:]\n",
    "        else:\n",
    "            series = series[start:end]\n",
    "        self.window = window\n",
    "        self.horizon = horizon\n",
    "        self.series = series\n",
    "        self.max_start = len(series) - window - horizon + 1\n",
    "        if self.max_start < 1:\n",
    "            raise ValueError(\"Window + horizon larger than available series length\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_start\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        start = idx\n",
    "        past = self.series[start : start + self.window]\n",
    "        future = self.series[start + self.window : start + self.window + self.horizon]\n",
    "        past = torch.from_numpy(past)  # shape: (window,)\n",
    "        future = torch.from_numpy(future)  # shape: (horizon,)\n",
    "        return past, future\n",
    "\n",
    "\n",
    "def build_dataloader(csv_path: str, \n",
    "                     window: int, \n",
    "                     horizon: int, \n",
    "                     batch_size: int = 32, \n",
    "                     shuffle: bool = True):\n",
    "    \"\"\"Create a DataLoader emitting `(past, horizon)` batches.\"\"\"\n",
    "    dataset = ForecastingDataset(csv_path=csv_path, \n",
    "                                 window=window, \n",
    "                                 horizon=horizon)\n",
    "    train_dataset = ForecastingDataset(csv_path=csv_path, \n",
    "                                       window=window, \n",
    "                                       horizon=horizon,\n",
    "                                       end=len(dataset) // 5)\n",
    "    train_dl = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=shuffle, \n",
    "                                           drop_last=False)\n",
    "    valid_dataset = ForecastingDataset(csv_path=csv_path, \n",
    "                                       window=window, \n",
    "                                       horizon=horizon,\n",
    "                                       start=len(dataset) // 5)\n",
    "    valid_dl = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=shuffle, \n",
    "                                           drop_last=False)\n",
    "    return train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9f94e1",
   "metadata": {},
   "source": [
    "## Part 2: First models\n",
    "\n",
    "In this part, you will build your first few forecasting models, train them, and\n",
    "experiment with classical detrending techniques used in time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b0b421",
   "metadata": {},
   "source": [
    "**Question 4.** Implement a simple autoregressive (AR) model in `torch`.\n",
    "\n",
    "The model should:\n",
    "- take a past window of shape `(batch, window)`\n",
    "- output a forecast of shape `(batch, horizon)`\n",
    "- be linear in the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2114d9",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "class ARModel(torch.nn.Module):\n",
    "    \"\"\"Linear autoregressive model.\"\"\"\n",
    "\n",
    "    def __init__(self, window: int, horizon: int):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(window, horizon)\n",
    "\n",
    "    def forward(self, past):\n",
    "        # past: (batch, window)\n",
    "        # output: (batch, horizon)\n",
    "        return self.linear(past)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1caac59",
   "metadata": {},
   "source": [
    "**Question 5.** Train the AR model using mean squared error (MSE) and evaluate\n",
    "it on the validation set.\n",
    "\n",
    "Implement:\n",
    "- a training loop\n",
    "- a validation loop\n",
    "- reporting of train and validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c129f56",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for past, future in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(past)\n",
    "        loss = criterion(pred, future)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * past.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for past, future in dataloader:\n",
    "        pred = model(past)\n",
    "        loss = criterion(pred, future)\n",
    "        total_loss += loss.item() * past.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def train_and_valid_loop(model, train_dl, valid_dl, optimizer, criterion, n_epochs):\n",
    "    logs = {\"train_loss\": [], \"valid_loss\": []}\n",
    "    print(model.__class__.__name__)\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_epoch(model, train_dl, optimizer, criterion)\n",
    "        logs[\"train_loss\"].append(train_loss)\n",
    "        valid_loss = eval_epoch(model, valid_dl, criterion)\n",
    "        logs[\"valid_loss\"].append(valid_loss)\n",
    "        print(f\"Epoch {epoch:02d} | train={train_loss:.4f} | valid={valid_loss:.4f}\")\n",
    "    return logs\n",
    "\n",
    "# Example usage\n",
    "window = 96\n",
    "horizon = 24\n",
    "n_epochs = 50\n",
    "\n",
    "train_dl, valid_dl = build_dataloader(\n",
    "    csv_path=\"data/ETTh1.csv\",\n",
    "    window=window,\n",
    "    horizon=horizon,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "ar_model = ARModel(window, horizon)\n",
    "optimizer = torch.optim.Adam(ar_model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "logs_AR = train_and_valid_loop(ar_model, train_dl, valid_dl, optimizer, criterion, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48af8cf",
   "metadata": {},
   "source": [
    "**Question 6.** Add input/output standardization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef734046",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "class StandardScaler(torch.nn.Module):\n",
    "    \"\"\"Fixed standardization layer.\"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"mean\", mean)\n",
    "        self.register_buffer(\"std\", std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean) / (self.std + 1e-6)\n",
    "\n",
    "    def inverse(self, x):\n",
    "        return x * (self.std + 1e-6) + self.mean\n",
    "\n",
    "def compute_scaler(dataloader):\n",
    "    values = []\n",
    "    for past, future in dataloader:\n",
    "        values.append(past[:, :1])\n",
    "    values = torch.cat(values, dim=0)\n",
    "    mean = values.mean()\n",
    "    std = values.std()\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "class ScaledARModel(ARModel):\n",
    "    def __init__(self, window, horizon, mean, std):\n",
    "        super().__init__(window, horizon)\n",
    "        self.scaler = StandardScaler(mean, std)\n",
    "\n",
    "    def forward(self, past):\n",
    "        past_scaled = self.scaler(past)\n",
    "        pred_scaled = self.linear(past_scaled)\n",
    "        return self.scaler.inverse(pred_scaled)\n",
    "\n",
    "# Compute scaler from training data only\n",
    "mean, std = compute_scaler(train_dl)\n",
    "scaled_ar_model = ScaledARModel(window, horizon, mean, std)\n",
    "optimizer = torch.optim.Adam(scaled_ar_model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "logs_scaledAR = train_and_valid_loop(scaled_ar_model, train_dl, valid_dl, optimizer, criterion, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38758a4",
   "metadata": {},
   "source": [
    "**Question 7.** Implement a differencing layer that removes local trends:\n",
    "\n",
    "  $x_t' = x_t - x_{t-1}$\n",
    "\n",
    "Then implement the inverse operation (integration) to recover forecasts\n",
    "back to the original scale.\n",
    "\n",
    "Experiment with how differencing affects convergence and final error, and \n",
    "compare forecasts qualitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c5121",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "class Differencing(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        # x: (batch, window)\n",
    "        return x[:, 1:] - x[:, :-1]\n",
    "\n",
    "\n",
    "class Integration(torch.nn.Module):\n",
    "    def forward(self, last_value, diffs):\n",
    "        # last_value: (batch, 1)\n",
    "        # diffs: (batch, horizon)\n",
    "        return last_value + torch.cumsum(diffs, dim=1)\n",
    "\n",
    "class DifferencedScaledARModel(torch.nn.Module):\n",
    "    def __init__(self, window, horizon, mean, std):\n",
    "        super().__init__()\n",
    "        self.scaler = StandardScaler(mean, std)\n",
    "        self.diff = Differencing()\n",
    "        self.ar = ARModel(window - 1, horizon)\n",
    "        self.integrate = Integration()\n",
    "\n",
    "    def forward(self, past):\n",
    "        scaled_past = self.scaler(past)\n",
    "        last_value = scaled_past[:, -1:].detach()\n",
    "        diffs = self.diff(scaled_past)\n",
    "        pred_diffs = self.ar(diffs)\n",
    "        scaled_preds = self.integrate(last_value, pred_diffs)\n",
    "        return self.scaler.inverse(scaled_preds)\n",
    "\n",
    "# Compute scaler from training data only\n",
    "mean, std = compute_scaler(train_dl)\n",
    "diff_scaled_ar_model = DifferencedScaledARModel(window, horizon, mean, std)\n",
    "optimizer = torch.optim.Adam(diff_scaled_ar_model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "logs_DifferencedScaledAR = train_and_valid_loop(diff_scaled_ar_model, train_dl, valid_dl, optimizer, criterion, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952155d3",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "def viz_forecast(list_models: list, dataloader: torch.utils.data.DataLoader, ts_id: int = 0):\n",
    "    past_val, future_val = next(iter(dataloader))\n",
    "    plt.plot(numpy.arange(past_val.shape[1]),\n",
    "            past_val[ts_id],\n",
    "            label=\"Past window\"\n",
    "    )\n",
    "    plt.plot(past_val.shape[1] + numpy.arange(future_val.shape[1]),\n",
    "            future_val[ts_id],\n",
    "            label=\"Ground truth horizon\"\n",
    "    )\n",
    "    for model in list_models:\n",
    "        pred_val = model(past_val).detach().numpy()\n",
    "        plt.plot(past_val.shape[1] + numpy.arange(pred_val.shape[1]),\n",
    "                pred_val[ts_id],\n",
    "                label=f\"Forecast ({model.__class__.__name__})\"\n",
    "        )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "viz_forecast([diff_scaled_ar_model, scaled_ar_model], valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708bfa9c",
   "metadata": {},
   "source": [
    "**Question 8.** Replace the linear layer in your best-performing AR model with MLPs.\n",
    "\n",
    "Compare:\n",
    "- convergence speed\n",
    "- validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a340ac",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim, hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DifferencedScaledDeepARModel(DifferencedScaledARModel):\n",
    "    def __init__(self, window, horizon, mean, std):\n",
    "        super().__init__(window, horizon, mean, std)\n",
    "        self.ar = MLP(window - 1, horizon)\n",
    "\n",
    "# Compute scaler from training data only\n",
    "mean, std = compute_scaler(train_dl)\n",
    "diff_scaled_deep_ar_model = DifferencedScaledDeepARModel(window, horizon, mean, std)\n",
    "optimizer = torch.optim.Adam(diff_scaled_deep_ar_model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "logs_DifferencedScaledDeepAR = train_and_valid_loop(diff_scaled_deep_ar_model, train_dl, valid_dl, optimizer, criterion, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1b383",
   "metadata": {},
   "source": [
    "**Wrap-up question.** Which factor had the largest impact on performance?\n",
    "1. model complexity\n",
    "2. scaling\n",
    "3. differencing\n",
    "\n",
    "Justify your answer with quantitative results and/or plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4ae1d",
   "metadata": {
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "plt.plot(logs_AR[\"valid_loss\"], label=\"AR\")\n",
    "plt.plot(logs_scaledAR[\"valid_loss\"], label=\"Scaled AR\")\n",
    "plt.plot(logs_DifferencedScaledAR[\"valid_loss\"], label=\"Differenced + Scaled AR\")\n",
    "plt.plot(logs_DifferencedScaledDeepAR[\"valid_loss\"], label=\"Differenced + Scaled Deep AR\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
