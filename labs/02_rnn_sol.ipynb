{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8669829",
   "metadata": {},
   "source": [
    "# RNNs for Time Series Forecasting\n",
    "\n",
    "In this lab, you will explore recurrent models (RNN, GRU, LSTM, xLSTM) and\n",
    "temporal convolutions (CNN/TCN) for forecasting. You will work on two tasks:\n",
    "1) a synthetic long-range-dependency dataset, and 2) the multivariate ETTh1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48e31e7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca80d9a1",
   "metadata": {},
   "source": [
    "## Part 1 — Toy data for long-term memory experiments\n",
    "\n",
    "We build a synthetic sequence where a short spike far in the past determines a\n",
    "pattern in the future (e.g., a sine burst at a fixed lag). This stresses\n",
    "long-range credit assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6665ec74",
   "metadata": {},
   "source": [
    "The dataset below is composed of sequences with:\n",
    "- a base noisy sine\n",
    "- a unique spike placed uniformly in the first half\n",
    "- a sine is located in the target at a fixed lag \n",
    "  after the spike occurred in the base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eed018",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SpikeLagDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Synthetic long-range-dependency dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, length=200, lag=80, n_samples=5000, noise=0.05):\n",
    "        self.length = length\n",
    "        self.lag = lag\n",
    "        self.n_samples = n_samples\n",
    "        self.noise = noise\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = torch.linspace(0, 4 * math.pi, self.length)\n",
    "        base = torch.sin(t) + self.noise * torch.randn_like(t)\n",
    "        spike_pos = torch.randint(10, self.length // 2, (1,)).item()\n",
    "        spike = torch.zeros_like(base)\n",
    "        spike[spike_pos] = 3.0\n",
    "        series = base + spike\n",
    "        target = torch.zeros(self.length)\n",
    "        target[spike_pos + self.length // 4 :spike_pos + self.length // 2] = torch.sin(\n",
    "            t[:self.length // 4]\n",
    "        )\n",
    "        return series.unsqueeze(-1), target.unsqueeze(-1)\n",
    "\n",
    "def build_spike_dataloaders(length=200, lag=80, batch_size=64, n_train=4000, n_valid=500):\n",
    "    train_ds = SpikeLagDataset(length=length, lag=lag, n_samples=n_train)\n",
    "    valid_ds = SpikeLagDataset(length=length, lag=lag, n_samples=n_valid)\n",
    "    train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\n",
    "    return train_dl, valid_dl\n",
    "train_dl, valid_dl = build_spike_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9d32e6",
   "metadata": {},
   "source": [
    "**Question.** Visualize a few series from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f246d",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "def plot_spike_samples(dataset, n=3):\n",
    "    fig, axs = plt.subplots(n, 1, figsize=(8, 6))\n",
    "    for i in range(n):\n",
    "        x, y = dataset[i]\n",
    "        axs[i].plot(range(len(x)), x.numpy(), label=\"input\")\n",
    "        axs[i].plot(range(len(x), len(x) + len(y)), y.numpy(), label=\"target\")\n",
    "        axs[i].legend()\n",
    "    plt.show()\n",
    "\n",
    "length, lag, n_train = 200, 80, 10\n",
    "dataset = SpikeLagDataset(length=length, lag=lag, n_samples=n_train)\n",
    "plot_spike_samples(dataset, n_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f820d4",
   "metadata": {},
   "source": [
    "**Question.** Implement a simple RNN forecaster:\n",
    "- Use `nn.RNN`, process the full sequence, and map the final hidden state to\n",
    "  the target segment with a linear head.\n",
    "- Report train/valid loss (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46949591",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[python]"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)  # pred: (batch, horizon, 1)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for x, y in dataloader:\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def train_and_valid_loop(model, train_dl, valid_dl, optimizer, criterion, n_epochs):\n",
    "    logs = {\"train_loss\": [], \"valid_loss\": []}\n",
    "    print(model.__class__.__name__)\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_epoch(model, train_dl, optimizer, criterion)\n",
    "        logs[\"train_loss\"].append(train_loss)\n",
    "        valid_loss = eval_epoch(model, valid_dl, criterion)\n",
    "        logs[\"valid_loss\"].append(valid_loss)\n",
    "        print(f\"Epoch {epoch:02d} | train={train_loss:.4f} | valid={valid_loss:.4f}\")\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bab9a5",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "class RNNForecaster(torch.nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden=64, horizon=200):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden, batch_first=True)\n",
    "        self.horizon = horizon\n",
    "        self.head = torch.nn.Linear(hidden, horizon)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, h = self.rnn(x)  # h: (1, batch, hidden)\n",
    "        h_last = h[0]\n",
    "        pred = self.head(h_last)\n",
    "        return pred.unsqueeze(-1)  # (batch, horizon, 1)\n",
    "\n",
    "hidden_dim = 64\n",
    "rnn = RNNForecaster(hidden=hidden_dim, horizon=length)\n",
    "opt = torch.optim.Adam(rnn.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "logs_rnn = train_and_valid_loop(rnn, train_dl, valid_dl, opt, criterion, n_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144ee7e",
   "metadata": {},
   "source": [
    "**Question.** Replace the RNN with GRU and LSTM. Compare:\n",
    "- convergence speed and final validation loss\n",
    "- qualitative forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f727a4e",
   "metadata": {
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "class GRUForecaster(RNNForecaster):\n",
    "    def __init__(self, input_dim=1, hidden=64, horizon=200):\n",
    "        super().__init__(input_dim, hidden, horizon)\n",
    "        self.rnn = torch.nn.GRU(input_dim, hidden, batch_first=True)\n",
    "\n",
    "\n",
    "class LSTMForecaster(RNNForecaster):\n",
    "    def __init__(self, input_dim=1, hidden=64, horizon=200):\n",
    "        super().__init__(input_dim, hidden, horizon)\n",
    "        self.rnn = torch.nn.LSTM(input_dim, hidden, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (h, c) = self.rnn(x)  # h: (1, batch, hidden)\n",
    "        h_last = h[0]\n",
    "        pred = self.head(h_last)\n",
    "        return pred.unsqueeze(-1)  # (batch, horizon, 1)\n",
    "\n",
    "hidden_dim = 64\n",
    "gru = GRUForecaster(hidden=hidden_dim, horizon=length)\n",
    "opt = torch.optim.Adam(gru.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "logs_gru = train_and_valid_loop(gru, train_dl, valid_dl, opt, criterion, n_epochs=20)\n",
    "\n",
    "hidden_dim = 64\n",
    "lstm = LSTMForecaster(hidden=hidden_dim, horizon=length)\n",
    "opt = torch.optim.Adam(lstm.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "logs_lstm = train_and_valid_loop(lstm, train_dl, valid_dl, opt, criterion, n_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e68bf5",
   "metadata": {},
   "source": [
    "The code below provides implementation for a TCN block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735232d",
   "metadata": {
    "title": "[python]"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "\n",
    "class CausalConv(torch.nn.Module):\n",
    "    \"\"\"Minimal 1D causal convolution block without activation.\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, k=3, d=1):\n",
    "        super().__init__()\n",
    "        self.pad = (k - 1) * d\n",
    "        self.conv = weight_norm(torch.nn.Conv1d(in_ch, out_ch, k, dilation=d))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.pad(x, (self.pad, 0))\n",
    "        return self.conv(x)\n",
    "\n",
    "class TCNBlock(torch.nn.Module):\n",
    "    \"\"\"A TCN block involving 2 dilated convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, k=3, d1=1, d2=2, dropout_rate=.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = CausalConv(in_ch, out_ch, k, d1)\n",
    "        self.conv2 = CausalConv(out_ch, out_ch, k, d2)\n",
    "        # 1x1 conv to project residual to the correct number of channels\n",
    "        self.conv_1x1 = torch.nn.Conv1d(in_ch, out_ch, kernel_size=1)\n",
    "        self.do1 = torch.nn.Dropout1d(p=dropout_rate)\n",
    "        self.do2 = torch.nn.Dropout1d(p=dropout_rate)\n",
    "        self.act = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.conv_1x1(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.act(out)\n",
    "        out = self.do1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.act(out)\n",
    "        out = self.do2(out)\n",
    "        return self.act(out + residual)  # (batch, channels, time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddbd687",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "**Question.** Based on the above implementation, try the following TCN baselines\n",
    "for your forecasting problem:\n",
    "- 1D causal CNN with dilations (a 1-block TCN).\n",
    "- Stack several TCN blocks such that the receptive field is sufficient for the task at stake.\n",
    "Compare against RNN/GRU/LSTM on the spike-lag task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d3864",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TCNForecaster(torch.nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden=64, n_blocks=3, k=3, horizon=200):\n",
    "        super().__init__()\n",
    "        blocks = []\n",
    "        ch_in = input_dim\n",
    "        for i in range(n_blocks):\n",
    "            blocks.append(TCNBlock(ch_in, hidden, k, d1=2 ** (2*i), d2=2 ** (2*i+1)))\n",
    "            ch_in = hidden\n",
    "        self.net = torch.nn.Sequential(*blocks)\n",
    "        self.head = torch.nn.Linear(hidden, horizon)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # (batch, channels, time)\n",
    "        feats = self.net(x)\n",
    "        last_feat = feats[:, :, -1]\n",
    "        pred = self.head(last_feat)\n",
    "        return pred.unsqueeze(-1)\n",
    "\n",
    "hidden_dim = 64\n",
    "tcn = TCNForecaster(hidden=hidden_dim, horizon=length)\n",
    "opt = torch.optim.Adam(tcn.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "logs_tcn = train_and_valid_loop(tcn, train_dl, valid_dl, opt, criterion, n_epochs=20)\n",
    "\n",
    "dict_logs = {\n",
    "    \"TCN\": logs_tcn,\n",
    "    \"RNN\": logs_rnn,\n",
    "    \"LSTM\": logs_lstm,\n",
    "    \"GRU\": logs_gru\n",
    "}\n",
    "for label, logs in dict_logs.items():\n",
    "    plt.plot(logs[\"valid_loss\"], label=label)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dict_models = {\n",
    "    \"TCN\": tcn,\n",
    "    \"RNN\": rnn,\n",
    "    \"LSTM\": lstm,\n",
    "    \"GRU\": gru\n",
    "}\n",
    "for i in range(3):\n",
    "    past, future = dataset[i]\n",
    "    plt.figure()\n",
    "    plt.plot(range(length), past.numpy())\n",
    "    plt.plot(range(length, 2*length), future.numpy(), label=\"Ground truth\")\n",
    "    for label, model in dict_models.items():\n",
    "        pred = model(past.unsqueeze(0))[0]\n",
    "        plt.plot(range(length, 2*length), pred.flatten().detach().numpy(), label=label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e3f96",
   "metadata": {},
   "source": [
    "## Part 2 — Multivariate ETTh1 forecasting\n",
    "\n",
    "We now revisit ETTh1 used in the previous lab session but now\n",
    "treat it as a multivariate-to-univariate \n",
    "forecasting problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e118ca72",
   "metadata": {},
   "source": [
    "Below is an adaptation from last session's `ETTh1Dataset` class in which we:\n",
    "- Allow specifying `window`, `horizon`, and optional time-of-day encoding\n",
    "  (scaled to [0, 1]).\n",
    "- Return tensors shaped `(batch, time, features)` where features>1 for input \n",
    "  tensors and features=1 for output tensor (target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aac8ef",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[python]"
   },
   "outputs": [],
   "source": [
    "def load_etth1(csv_path, use_time_feat=True):\n",
    "    def to_str(str_or_bytes):\n",
    "        if isinstance(str, str_or_bytes):\n",
    "            return str_or_bytes\n",
    "        else:\n",
    "            return str_or_bytes.decode()\n",
    "    \n",
    "    d_conv = {\n",
    "        0: (lambda x: float(to_str(x).split(\" \")[1].split(\":\")[0]))\n",
    "    }\n",
    "    raw = numpy.loadtxt(csv_path, delimiter=\",\", skiprows=1, converters=d_conv)\n",
    "    features = raw.astype(numpy.float32)\n",
    "    if use_time_feat:\n",
    "        features[:, 0] /= 23.\n",
    "    else:\n",
    "        features = features[:, 1:]\n",
    "    return features\n",
    "\n",
    "\n",
    "class ETTh1Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, window, horizon, use_time_feat=True, start=0, end=None, mean=None, std=None):\n",
    "        feats = load_etth1(csv_path, use_time_feat=use_time_feat)\n",
    "        feats = feats[start:]\n",
    "        if end is not None:\n",
    "            feats = feats[:end]\n",
    "        self.feats = feats\n",
    "        self.window = window\n",
    "        self.horizon = horizon\n",
    "        self.max_start = len(feats) - window - horizon + 1\n",
    "        if self.max_start < 1:\n",
    "            raise ValueError(\"Window + horizon exceeds series length\")\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_start\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        past = self.feats[idx : idx + self.window]\n",
    "        future = self.feats[idx + self.window : idx + self.window + self.horizon, -1:]\n",
    "        # Apply scaling if mean and std are provided\n",
    "        if self.mean is not None and self.std is not None:\n",
    "            past = (past - self.mean) / self.std\n",
    "            future = (future - self.mean[-1:]) / self.std[-1:]\n",
    "        return torch.from_numpy(past), torch.from_numpy(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a19559",
   "metadata": {},
   "source": [
    "**Question.** Based on the `ETTh1Dataset` class provided above, implement \n",
    "a `build_etth1_dataloaders` function that generates training and validation \n",
    "data loaders, with scaling. Load the data relying on a past window of size 96 \n",
    "and a horizon of 24 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d041f",
   "metadata": {
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "def build_etth1_dataloaders(csv_path, window=96, horizon=24, batch_size=64, split=0.8):\n",
    "    dataset = ETTh1Dataset(csv_path, window, horizon)\n",
    "    n = len(dataset)\n",
    "    n_train = int(split * n)\n",
    "    train_ds = ETTh1Dataset(csv_path, window, horizon, end=n_train)\n",
    "    mean = numpy.mean(train_ds.feats, axis=0)  # (n_features,)\n",
    "    std = numpy.std(train_ds.feats, axis=0)  # (n_features,)\n",
    "    std = numpy.where(std < 1e-8, 1.0, std)\n",
    "    \n",
    "    # Create datasets with scaling\n",
    "    train_ds = ETTh1Dataset(csv_path, window, horizon, end=n_train, mean=mean, std=std)\n",
    "    valid_ds = ETTh1Dataset(csv_path, window, horizon, start=n_train, mean=mean, std=std)\n",
    "    train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\n",
    "    return train_dl, valid_dl\n",
    "\n",
    "window = 96\n",
    "horizon = 24\n",
    "dataset = ETTh1Dataset(\"data/ETTh1.csv\", window=window, horizon=horizon)\n",
    "train_dl, valid_dl = build_etth1_dataloaders(\"data/ETTh1.csv\", window=window, horizon=horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25f206",
   "metadata": {},
   "source": [
    "**Question.** Re-use TCN and GRU forecasting models from part 1 and \n",
    "evaluate their performance on this dataset. \n",
    "Does the TCN's receptive field cover the whole input window in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5876a8b0",
   "metadata": {
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "channels = 8\n",
    "horizon = 24\n",
    "gru = GRUForecaster(input_dim=channels, hidden=hidden_dim, horizon=horizon)\n",
    "opt = torch.optim.Adam(gru.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "logs_gru = train_and_valid_loop(gru, train_dl, valid_dl, opt, criterion, n_epochs=20)\n",
    "\n",
    "hidden_dim = 64\n",
    "tcn = TCNForecaster(input_dim=channels, hidden=hidden_dim, horizon=horizon)\n",
    "opt = torch.optim.Adam(tcn.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "logs_tcn = train_and_valid_loop(tcn, train_dl, valid_dl, opt, criterion, n_epochs=20)\n",
    "\n",
    "dict_logs = {\n",
    "    \"TCN\": logs_tcn,\n",
    "    \"GRU\": logs_gru\n",
    "}\n",
    "for label, logs in dict_logs.items():\n",
    "    plt.plot(logs[\"valid_loss\"], label=label)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_models = {\n",
    "    \"TCN\": tcn,\n",
    "    \"GRU\": gru\n",
    "}\n",
    "it = iter(valid_dl)\n",
    "for i in range(3):\n",
    "    past, future = next(it)\n",
    "    plt.figure()\n",
    "    plt.plot(range(window), past[0, :, -1].numpy())\n",
    "    plt.plot(range(window, window + horizon), future[0].numpy(), label=\"Ground truth\")\n",
    "    for label, model in dict_models.items():\n",
    "        pred = model(past)[0]\n",
    "        plt.plot(range(window, window + horizon), pred.flatten().detach().numpy(), label=label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5926afd",
   "metadata": {},
   "source": [
    "**Question.** Now you will use the code from the `xlstm` package to build an xLSTM\n",
    "model for this forecasting task. What stack of sLSTM/mLSTM blocks is used here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2607310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xlstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253033e6",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[python]"
   },
   "outputs": [],
   "source": [
    "from xlstm.xlstm_block_stack import (\n",
    "    xLSTMBlockStack,\n",
    "    xLSTMBlockStackConfig,\n",
    ")\n",
    "from xlstm.blocks.mlstm.block import mLSTMBlockConfig\n",
    "from xlstm.blocks.mlstm.layer import mLSTMLayerConfig\n",
    "\n",
    "class xLSTMForecaster(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_blocks: int,\n",
    "        window: int,\n",
    "        horizon: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Project input channels → model dimension\n",
    "        self.input_proj = torch.nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Build block configs (repeat to get depth)\n",
    "        layer_config = mLSTMLayerConfig(\n",
    "            embedding_dim=hidden_dim,\n",
    "            num_heads=2,\n",
    "        )\n",
    "        block_cfg = mLSTMBlockConfig(\n",
    "            mlstm=layer_config,\n",
    "        )\n",
    "\n",
    "        xlstm_cfg = xLSTMBlockStackConfig(\n",
    "            mlstm_block=block_cfg,\n",
    "            slstm_at=[],\n",
    "            num_blocks=num_blocks,\n",
    "            context_length=window,\n",
    "            embedding_dim=hidden_dim\n",
    "        )\n",
    "\n",
    "        self.xlstm = xLSTMBlockStack(xlstm_cfg)\n",
    "\n",
    "        # Forecast head\n",
    "        self.head = torch.nn.Linear(hidden_dim, horizon)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)        # (B, T, c)\n",
    "        y = self.xlstm(x)             # (B, T, c)\n",
    "\n",
    "        last_state = y[:, -1]         # (B, c)\n",
    "        out = self.head(last_state)   # (B, H)\n",
    "\n",
    "        return out.unsqueeze(-1)      # (B, H, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e869c23",
   "metadata": {},
   "source": [
    "**Question.** Train your own xLSTM forecaster and check its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc949d2e",
   "metadata": {
    "tags": [
     "solution"
    ],
    "title": "+"
   },
   "outputs": [],
   "source": [
    "xlstm_model = xLSTMForecaster(\n",
    "    input_dim=channels,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_blocks=1,\n",
    "    horizon=horizon,\n",
    "    window=window\n",
    ")\n",
    "opt = torch.optim.Adam(xlstm_model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "logs_xlstm = train_and_valid_loop(xlstm_model, train_dl, valid_dl, opt, criterion, n_epochs=20)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
