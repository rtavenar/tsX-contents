# ðŸ”¬ Lab Ideas for Session 4: Attention Mechanisms and Transformers for Time Series

## Lab 4.1: From RNNs to Attention + Interpretability
- **Task:** Take an RNN-based forecasting model and augment it with an attention mechanism (Bahdanau/Luong-style).  
- **Focus:** Compare performance and interpretability of attention vs. plain RNN.  
- **Advanced Twist:**  
  - Visualize attention weights over time and analyze whether the model correctly focuses on seasonal or recent patterns.  
  - Investigate whether attention highlights meaningful temporal regions (e.g., seasonal cycles, recent shocks).  
  - Run counterfactual experiments (e.g., permuting time lags, injecting anomalies) to test if attention weights reflect real importance or are artifacts.  

---

## Lab 4.2: Implementing a Transformer for Time Series
- **Task:** Implement a vanilla Transformer encoder-decoder for forecasting on a real-world dataset.  
- **Focus:** Study scalability, training stability, and effectiveness compared to RNN/CNN baselines.  
- **Advanced Twist:** Experiment with modifications (e.g., causal masking, positional encodings) and analyze their effect on forecasting performance.  

---

## Lab 4.3: Time-Aware Embeddings
- **Task:** Incorporate time-related features into a Transformer (e.g., timestamps, day-of-week, holiday indicators).  
- **Focus:** Evaluate the impact of temporal embeddings on model accuracy.  
- **Advanced Twist:** Compare different embedding schemes (sinusoidal, learned embeddings, relative time encodings) and analyze which best captures seasonality and irregular intervals.  

---

## Lab 4.4: Long-Range Forecasting with Efficient Transformers
- **Task:** Implement and benchmark efficient Transformer variants (e.g., Informer, LogSparse, Performer, or Longformer) on long-horizon forecasting tasks.  
- **Focus:** Compare training time, memory efficiency, and accuracy against vanilla Transformers and RNNs.  
- **Advanced Twist:** Stress-test models with extremely long sequences and analyze at what sequence length RNNs fail vs. Transformers succeed.  
