# ðŸ”¬ Lab Ideas for Session 2: Convolutional and Recurrent Neural Networks

## Lab 2.1: RNN Variants for Sequence Modeling
- **Task:** Implement and train vanilla RNNs, LSTMs, and GRUs on the same dataset.  
- **Focus:** Compare convergence speed, stability, and forecasting accuracy.  
- **Advanced Twist:** Analyze hidden state dynamics by visualizing activations over time and detecting vanishing/exploding gradient behaviors.  

---

## Lab 2.2: CNNs vs. RNNs on Sequence Length and Long-Range Dependencies
- **Task:** Train CNNs and RNNs with varying input sequence lengths (short, medium, long) on the same forecasting problem.  
- **Focus:** Compare how CNNs and RNNs capture dependencies, trade-offs in accuracy, speed, and memory.  
- **Advanced Twist:**  
  - Introduce **synthetic long-range dependencies** (e.g., periodic spikes every 200 steps) and test which architectures capture them.  
  - Explore dilated convolutions or gated RNNs as enhancements.  
  - Build a **hybrid CNN+RNN model** and evaluate if combining local feature extraction with sequential modeling improves performance.  

---

## Lab 2.3: Robustness to Noise and Missing Data
- **Task:** Corrupt the input sequences with random noise and missing values, then train CNNs and RNNs.  
- **Focus:** Evaluate robustness of architectures to imperfect data.  
- **Advanced Twist:** Implement simple imputation strategies (mean fill, forward fill) vs. letting the network implicitly handle missingness, and compare.  
