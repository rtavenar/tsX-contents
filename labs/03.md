# ðŸ”¬ Lab Ideas for Session 3: Probabilistic Forecasting and Uncertainty Quantification

## Lab 3.1: Deterministic vs. Probabilistic Forecasting
- **Task:** Train a standard deterministic forecasting model (e.g., MSE-trained RNN) and a probabilistic model (e.g., quantile regression or Gaussian likelihood loss).  
- **Focus:** Compare point forecasts vs. full predictive distributions.  
- **Advanced Twist:** Evaluate calibration quality by checking whether predicted intervals match empirical coverage probabilities.  

---

## Lab 3.2: Implementing Distributional Losses
- **Task:** Implement probabilistic losses such as:  
  - Negative log-likelihood (Gaussian, Laplace)  
  - Quantile (pinball) loss  
  - CRPS (Continuous Ranked Probability Score)  
- **Focus:** Understand how different loss choices shape predictive distributions.  
- **Advanced Twist:** Require students to visualize predictive distributions (histograms, KDEs, fan charts) for different methods and discuss overconfidence vs. underconfidence.  

---

## ~~Lab 3.3: Bayesian Deep Learning for Forecasting~~
- **Task:** Use dropout at inference (MC Dropout) or deep ensembles to quantify epistemic uncertainty in a forecasting model.  
- **Focus:** Explore the difference between aleatoric (data-driven) and epistemic (model-driven) uncertainty.  
- **Advanced Twist:** Compare MC Dropout vs. deep ensembles vs. deterministic baselines in terms of calibration, computational cost, and predictive sharpness.  

---

## Lab 3.4: Out-of-Distribution Detection with Predictive Uncertainty
- **Task:** Train a probabilistic model on one dataset, then evaluate it on shifted or corrupted versions of the data.  
- **Focus:** Assess whether higher predictive uncertainty correlates with out-of-distribution inputs.  
- **Advanced Twist:** Require students to quantify uncertainty under synthetic distribution shifts (e.g., add unseen seasonal pattern, scale variance) and evaluate if uncertainty estimates are reliable indicators of OOD inputs.  
